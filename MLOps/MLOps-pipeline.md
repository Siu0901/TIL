# [TIL] MLOps 자동화 파이프라인 뼈대 정리 (Airflow / MLflow / FastAPI / Docker)

대충 한줄 요약: **“학습은 필요할 때만 돌리고, 서빙은 항상 켜두고, 모델은 MLflow로 버전 관리한다.”**  
MLOps 자동화 파이프라인 **개념 및 전체 작동 방식**에 대해 공부해봄

## 궁금했던 거

- 로컬에서 만든 모델 학습 코드를
  - 매일 자동으로 학습시키고
  - 모델을 버전별로 저장/추적하고
  - API 서버가 최신 모델을 안정적으로 서빙하게 만들려면
- **구성요소를 어떻게 나누고**, **어떤 흐름으로 연결**해야하지?



## 전체 흐름

1. **Airflow**가 스케줄에 맞춰 학습 작업을 “시작”시킨다.  
2. 학습은 **훈련용 Docker 컨테이너**에서 돌고, 결과를 **MLflow**에 기록한다.
3. MLflow는
   - 실험 메타데이터(파라미터/지표)를 DB에,
   - 모델 파일(가중치 파일 같은거)을 S3 같은 스토리지에 저장하도록 구성할 수 있다.
4. **FastAPI**는 평소에 계속 켜져 있으면서
   - 시작할 때 최신 모델을 **한 번만 로드**해 메모리에 올리고,
   - 예측 요청이 올 때는 다운로드 없이 바로 예측만 한다.
5. 새로 모델 학습되어 나오면
   - FastAPI에 `/reload-model` 같은 엔드포인트를 호출해서
   - **서버 재시작 없이 모델만 갈아끼우는(무중단) 방식**을 쓸 수 있다.

대충 흐름 요약하면
{데이터 수집/처리/학습 - mlflow에 기록 - fastapi가 mlflow에 있는거 서빙} - 이 과정을 airflow로 짜는거임



## 각 구성요소 역할

### Airflow: 관리자, 일정 조율하는거
- 하는 거: “매일 새벽 3시에 학습 돌려” 같은 **스케줄 + 작업 순서 관리**

### MLflow: 모델 실험 기록 + 모델 레지스트리
- 하는 거:
  - 실험 지표, 파라미터, 모델 파일 기록
  - 모델 레지스트리로 모델 업뎃 관리
- 저장소 분리 개념:
  - 메타데이터(가벼운거) → DB
  - 모델 파일(무거운거) → S3

### FastAPI: 서빙 (api 요청 처리)
- 하는 거: 예측 요청 `/predict` 처리
- 모델 불러올때 주의사항:
  - **요청마다 모델 다운로드하면 속도 엄청 느려진다**
  - 서버 시작 시 1회 로드(lifespan)로 해결해야 함
  - 여기서 모델 업뎃 되면 새로 업뎃된 모델 로드하는 api 만들어야됨

### Docker / Compose: 환경 고정 + 네트워크 연결
- 하는 일:
  - 내 컴퓨터에서만 되는 문제 제거
  - compose로 여러 컨테이너를 한 번에 올리고 내부 네트워크 구성



## 비용 관점
학습, 서빙, 관리 대충 이런 애들 각각 도커로 감싸고 배포해야되서 서버 비용 문제가 좀 있을거라 생각해 이것도 함 알아봤음

### 서버 항상 켜둘 것
- Airflow(웹/스케줄러), MLflow 서버, FastAPI 서버  
- DB 등
> 트래픽/스케줄을 기다려야 하니 **저렴한 서버에서 항상**하는 편이라 함

### 서버 필요할 때만 켤 것
- GPU/고성능이 필요한 학습 작업
> Airflow가 스케줄에 맞춰 잠깐 띄우고, 끝나면 삭제해서 과금 최소화



## 프로젝트 디렉터리 구조 2가지

### A) 학습/로컬 테스트용 Monorepo(한 레포에 전부)
> 전체 흐름 이해 + 로컬 실습에 좋음

```text
my_mlops_project/
├── docker-compose.yml
├── .env
│
├── airflow/
│   └── dags/
│       └── daily_train_dag.py
│
├── training/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── train.py
│
└── serving/
    ├── Dockerfile
    ├── requirements.txt
    └── main.py
```

### B) 실무용 Multi-repo(역할별로 레포 분리한거)

팀 협업/권한 분리/배포 단위 분리에 좋음 (실제 회사에서 흔한 형태라고 함)

1) ml-model-training (AI/DS 팀)
```text
ml-model-training/
├── Dockerfile
├── requirements.txt
└── src/
    ├── extract.py
    ├── preprocess.py
    └── train.py

결과물: 훈련용 Docker 이미지(ECR/DockerHub 등에 push)
```

2) ml-api-serving (Backend/ML Eng 팀)
```text
ml-api-serving/
├── Dockerfile
├── requirements.txt
└── src/
    ├── main.py
    └── schemas.py

결과물: 서빙용 Docker 이미지 + 배포
```

3) mlops-airflow-infra (DevOps/Data Eng 팀)
```text
mlops-airflow-infra/
├── docker-compose.yml   # 또는 k8s yaml
├── dags/
│   └── daily_train_dag.py
└── scripts/
    └── bootstrap.sh
```

하는 일: 스케줄(DAG) + 인프라 설정 + 환경변수 주입

레포가 분리돼도 연결되는 이유는  도커 이미지 레지스트리 + 서비스 주소로 연결되기 때문이다.

## 느낀거
사랑스런 제미나이와 구글이랑 열띤 일방적 문답 끝에 드디어 MLOps에서 자동화 파이프라인 이게 어떤식으로 이뤄지는지 어느정도 알게 됬다. 생각한거 만큼 구조가 상당히 복잡한거 같은데 뭔가 막상 짜라 하면 짤 수 있을 거 같긴 함. 물론 학습이나 각 레포 구축 쪽 얘기고 배포 및 연결 관련해선 아직 도커로 감싸는거도 안해봐서 모르기 때문에 그쪽 공부는 더 해봐야할 것 같음. 

AI 엔지니어 공부를 하면 할수록 내가 AI를 하는건지 백엔드를 하는건지 데브옵스를 하는건지 모르겠다. 확실한건 AI는 약간 활용 느낌이고 백엔드쪽 기반으로 개발하는 느낌이 강한거 같음. 흔히 우리가 아는 백엔드는 erd 설계하고 전체적인 서비스의 로직 구축한다면 우린 AI 관련된 서비스 로직 구축 + 모델 평가, 학습 자동화 이런느낌? 물론 그렇다고 AI 소홀히 하면 안될 것 같긴 함.

요즘 AI 공부 안한지 오래되서 초심 잃은 거 같다. 전체적인건 아는데 세세히 구현 들어가면 llm에 의존하게되는 그런 상황임. 다시 한번 개념 복습 돌리고 딥러닝 프레임워크 다시 익히고,  프젝 할 때 오늘 배운거 적용해서 한번 자동화 이거 해보자. aws, 도커 등 배포 관련해서도 공부해보자.
