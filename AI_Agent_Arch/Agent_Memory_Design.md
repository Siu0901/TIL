# AI 에이전트 대화 메모리 설계

벡터 db 공부하려다 딴 길로 새서 AI 에이전트가 대화를 기억하게 하는 방법을 공부함. 
결론부터 말하면 **모델이 스스로 기억하는 게 아니라, 매 요청마다 필요한 기억을 다시 주입**하는 구조를 만든다는 것임.


## 메모리 종류들

### Short-term (최근 대화 버퍼)
- 최근 2~6턴 정도를 그대로 프롬프트에 넣어 현재 대화 내용을 유지시킴

### Summary memory (러닝 요약)
- 대화가 길어지면 오래된 부분을 요약해 압축해서 유지함
- 요약에는 배경설명보다 **결정/현재상태/미해결 할 일**만 남기는 게 좋음

### Long-term memory (장기 메모리, RAG)
- 벡터 DB에 저장해두고,
- 다음 질문이 오면 **질문 임베딩으로 관련 메모리만 검색** 해서 프롬프트에 넣음


## 장기 메모리에 저장할 때 메모리 단위로 저장

대화/질문 원문을 그대로 저장하면 잡담/노이즈가 많아 검색 품질이 떨어진다.  
그래서 저장할 때는 **원문에서 핵심만 뽑아 짧은 사실요약 카드? 비스무리하게 쪼개서 저장**함.

대충 예시)
- [선호] 사용자는 한국어로 답변을 원함
- [결정] 하이브리드 메모리(최근턴+요약+벡터검색) 사용
- [프로젝트 상태] 대화 메모리 시스템 구현 중
- [할 일] 메모리 추출 규칙/포맷 설계

이런 항목을 **1각각 DB 1레코드**로 저장해야 나중에 갖다 쓸때 매우 편함.


## LLM에 넣는 프롬프트 구성

매 요청마다 아래 순서로 구성하면 안정적임.

1. **System**: 역할/말투/규칙(짧게)
2. **Running Summary**: 전체 흐름 요약(짧게)
3. **Retrieved Memories**: DB에서 검색된 관련 기억 top-k
4. **Recent Turns**: 최근 대화 2~6턴
5. **Current Question**: 이번 사용자 질문

이게 토큰 아껴야되서 **많이 넣기보다 필요한 것만 짧게 넣기**가 중요함.


## 메모리 업데이트 파이프라인

### 답변 생성 전
- 사용자의 질문을 임베딩함
- vector DB에서 사용자 질문과 관련있는 이전 내용 검색함
- 위 프롬프트 구성으로 LLM 호출함

### 답변 생성 후
- LLM/규칙으로 저장 가치 있는 메모리 3~5개 추출함
- 메모리 항목별로 임베딩 후 저장/업데이트함
- recent_turns가 길어지면 running_summary 업데이트


## 중복 처리

DB에 넣을 때 이미 비슷한 내용이 있거나 그러면 따로 덮어쓰거나 삭제하거나 그렇게 해야함.
LLM으로 구분할 때 항상 쓰기보다는 아래 조합이 효율적이다.

### 임베딩 유사도로 중복 감지
- 새 메모리와 기존 메모리의 cosine similarity로 판단
  - 0.90+ : 거의 동일 -> 스킵/덮어쓰기
  - 0.80~0.90 : 애매 -> 업데이트 후보로 둠

### LLM은 뭔가 살짝 애매할때만
- 유사도 애매 구간(0.80~0.90) 또는 충돌 가능성이 있으면
- LLM에게 중복/업데이트/별개 판정과 병합한 문장 1개 생성만 시킴


## 정리
프롬프트 안에 내용들을 넣어 기억하는 것 처럼 만든다란 사실은 이미 알고 있었지만, 새부과정이 굉장히 많고 어떻게 하면 효율적으로 이걸
구성해서 넣을 지 처리하는 과정이 새로웠음. 그 대화 기억때도 rag를 이용해 기존 대화 내용과 관련된 대화 정보를 꺼내 프롬프트에 넣는 과정이 좀 신기했다 해야되나? 아무튼 그럼.

궁금한건 그 llm들 보면 새 채팅 생성해서 대화할 때 다른 채팅에서 대화했던 내용들을 알고있던데, 이런거는 따로 뭐 포괄적인 프롬프트나 저장하는 곳이이 있는지 궁금함. 함 알아보자.
